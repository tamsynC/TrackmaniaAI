<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="41118 Trackmania Portfolio">
  <meta name="keywords" content="41118">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>41118 Trackmania Portfolio</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">41118 Trackmania Portfolio</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Tamsyn Crangle</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Jet Webb</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="#">Nicolas Yao</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="#">Connor Williams</a><sup>4</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University Technology Sydney,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. 
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>-->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="#video"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tamsynC/TrackmaniaAI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/rFalque/41118_project-sample/tree/main/data_sample"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png"
            class="teaser-image"
            alt="Teaser."/>
      <h2 class="subtitle has-text-centered">
		Trackmania AI Flowchart
      </h2>
	  <p>
		This flowchart illustrates a reinforcement learning pipeline designed for a <strong>Trackmania AI agent</strong> that drives autonomously using visual input. The system is composed of four main components:
	  </p>

	  <ol style="padding-left: 1.5em; list-style-position: inside;"><br>
		<li>
		  <strong>Game Frame Input:</strong> A frame from the Trackmania simulation is captured, showing the car and the track environment.
		</li>
		<li>
		  <strong>CNN (Segmentation):</strong> This frame is passed into a <em>Convolutional Neural Network</em> trained for image segmentation. The CNN outputs a segmentation mask that highlights key visual features of the track such as road boundaries, curbs, and off-track areas.
		</li>
		<li>
		  <strong>DQN (Driving Policy):</strong> The segmentation mask is then fed into a <em>Deep Q-Network</em> (DQN), which uses it to determine the optimal driving action. The DQN is trained using reinforcement learning to maximize performance based on rewards.
		</li>
		<li>
		  <strong>Trackmania Simulation/Game Engine:</strong> The selected action is executed in the game engine, moving the car and generating a new frame along with a reward signal based on the car’s behavior (e.g., staying on track, speed, lap time). This reward feeds back into the DQN to improve decision-making.
		</li>
	  </ol>

	  <p><br>
		The loop continues with the next frame captured from the game, completing the feedback cycle of <strong>perception</strong>, <strong>decision-making</strong>, and <strong>learning</strong>. This process enables the AI to learn and improve its driving policy over time through interaction with the environment.
	  </p>
    </div>
  </div>
</section>

<style>
  .fixed-height-img {
    height: 250px; /* or any height you prefer */
    width: auto;
    object-fit: contain; /* or use 'cover' if you want to crop */
    display: block;
    margin: 0 auto;
  }
</style>
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Team members</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <img src="./static/images/JetW.jpg"
               class="fixed-height-img"
               alt="Jet Webb">
        </div>
        <div class="item">
          <img src="./static/images/ConnorW.jpeg"
               class="fixed-height-img"
               alt="Connor Williams">
        </div>
        <div class="item">
          <img src="./static/images/TamsynC.jpeg"
               class="fixed-height-img"
               alt="Tamsyn Crangle">
        </div>
        <div class="item">
          <img src="./static/images/NicolasY.jpeg"
               class="fixed-height-img"
               alt="Nicolas Yuen">
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
			This project focuses on developing an autonomous driving agent for Trackmania 2020 using reinforcement learning. The goal is to train an AI to complete tracks as quickly and efficiently as possible without human input. The system uses a Convolutional Neural Network (CNN) for real-time image segmentation of the game environment and a Deep Q-Network (DQN) to make driving decisions based on visual input. Through continuous interaction with the game, the AI learns to improve its lap times by optimizing its control strategy and navigating the track with increasing precision.          </p>
          <p>
          <p>
            The video shows where we got up to and explains how it works.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
 <!-- Video Section -->
	<div class="columns is-centered has-text-centered">
	  <div class="column is-full-width">
		<h2 id="video" class="title is-3">Video</h2>

		<div class="publication-video">
		  <iframe 
			width="560" 
			height="315" 
			src="https://www.youtube.com/embed/IWzbH_BySn8?showinfo=0" 
			frameborder="0" 
			allow="autoplay; encrypted-media" 
			allowfullscreen>
		  </iframe>
		</div>
	  </div>
	</div>

</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">CNN evaluation</h2>
          <p>
            Confusion matrix of the CNN evaluation on the test set.
          </p>

          <img src="./static/images/results/confusionmatrix.png"
            class="teaser-image"
            alt="Teaser."/>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">DQN training</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Sample of the reward convergence plot of the DQN training.
            </p>
            <img src="./static/images/results/reward_convergence_plot.png"
            class="teaser-image"
            alt="Teaser."/>
          </div>
        </div>
      </div>
    </div>
	
      <div class="columns is-centered has-text-centered">
        <div class="content">
          <h2 class="title is-3"><br>CNN Segmentation</h2>
          <p>
            Original vs segmented overlay on the test set.
          </p>

          <img src="./static/images/results/segmentedoverlay.png"
            class="teaser-image"
            alt="Teaser."/>
        </div>
      </div>
	  <div class="columns is-centered has-text-centered">
        <div class="content">
          <h2 class="title is-3"><br>Cumulative Rewards</h2>
          <p>
            Below shows the training decaying from too many punishments accumulating and the model giving up on learning. It also shows overfitting to short-term rewards.
			(Top graph: rewards per step, Bottom Graph: Cumulative Rewards Per Crash)
          </p>

          <img src="./static/images/decaying.png"
            class="teaser-image"
            alt="Teaser."/>
        </div>
      </div>
    <!--/ Matting. -->




    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results discussion and reflection</h2>

        <div class="content has-text-justified">
			<p>
			  Our custom reward function is designed to guide a Deep Q-Network (DQN) agent toward fast, efficient, and intelligent driving behavior. Rewards and penalties are allocated as follows:
			</p>

			<ul>
			  <li><strong>Checkpoint Reached:</strong> +25 points for each checkpoint.</li>
			  <li><strong>Consecutive Checkpoint Bonus:</strong> +25 points for each checkpoint in a streak.</li>
			  <li><strong>Speed Bonuses:</strong> Extra rewards based on time since last checkpoint:
				<ul>
				  <li><strong>&lt; 1.5s:</strong> +70 </li>
				  <li><strong>&lt; 3.0s:</strong> +40 </li>
				  <li><strong>&lt; 5.0s:</strong> +30 </li>
				  <li><strong>&lt; 6.0s:</strong> +20 </li>
				</ul>
			  </li>
			  <li><strong>Direction Bonus:</strong> Up to +5 points for staying aligned with the track.</li>
			  <li><strong>Race Completion:</strong> +1000 points for finishing the course.</li>
			</ul>

			<p><strong>Penalties:</strong></p>
			<ul>
			  <li><strong>Too Fast (Possible Cheating):</strong> -5 if checkpoint is reached in under 0.5s.</li>
			  <li><strong>Crash:</strong> -10 base, reduced by 2 for each checkpoint in a streak.</li>
			  <li><strong>Similarity Crash:</strong> -10 for repeated crash behavior.</li>
			  <li><strong>Stuck:</strong> -15 base, reduced by 3 for each consecutive checkpoint.</li>
			  <li><strong>Out of Bounds:</strong> -7 base, reduced by checkpoint streak.</li>
			  <li><strong>Timeout:</strong> -5 if too long since last checkpoint.</li>
			</ul>

			<p>
			  This dynamic reward structure encourages the agent to learn not just to reach checkpoints, but to do so quickly, smoothly, and consistently—while penalizing unsafe or inefficient actions.
			</p>


        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->
	<div class="columns is-centered">
	  <div class="column is-full-width">
		<h2 class="title is-3">DQN Action Selection</h2>

		<div class="content has-text-justified">
		  <p>
			The agent uses a Deep Q-Network (DQN) to decide which action to take at every step. The action selection balances exploration (random behavior) and exploitation (choosing the best-known action) based on a probability value <code>epsilon</code>.
		  </p>

		  <p><strong>Epsilon-Greedy Strategy:</strong></p>
		  <ul>
			<li><strong>Random Action:</strong> With probability <code>epsilon</code>, the agent selects a random action. This encourages exploration of new strategies.</li>
			<li><strong>Track-Aware Bias:</strong> Random actions are influenced by the <code>track_direction</code>. If the car is veering left or right, the random action is slightly biased to correct the path.</li>
			<li>Most often, forward (<code>W</code>) is selected randomly; sometimes backward (<code>S</code>) is chosen.</li>
		  </ul>

		  <p><strong>Neural Network Inference:</strong></p>
		  <ul>
			<li>With probability <code>1 - epsilon</code>, the agent uses the DQN to compute Q-values for each possible action.</li>
			<li>The input <code>state</code> is converted into a tensor and processed by the network.</li>
			<li>The network outputs values for each action, which are passed through a sigmoid function to convert them into probabilities.</li>
			<li>The output probabilities determine whether to:
			  <ul>
				<li>Turn <strong>left</strong> or <strong>right</strong> if probabilities exceed a high threshold (0.98).</li>
				<li>Move <strong>forward</strong> or <strong>backward</strong> based on the highest motion probability.</li>
			  </ul>
			</li>
			<li>All computations are optimized for GPU usage and memory efficiency.</li>
		  </ul>

		  <p>
			This approach ensures the agent learns to make reliable, context-aware decisions, while still occasionally exploring alternate behaviors to improve learning.
		  </p>
		</div>
	  </div>
	</div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">CNN Segmentation Model</h2>

        <div class="content has-text-justified">
          <p>
            The segmentation model used in this project is based on a U-Net architecture with a ResNet-34 encoder pretrained on ImageNet. This model is specifically designed for <strong>semantic segmentation</strong>, which involves assigning a class label to every pixel in an input image.
          </p>

          <p>
            The training data consists of <strong>Trackmania gameplay frames</strong> annotated using the COCO format. The dataset includes four object classes in addition to a background class. A custom PyTorch <code>Dataset</code> class is used to load the dataset, which parses COCO annotations and converts them into binary masks corresponding to each object category.
          </p>

          <p>
            Training is performed using the <code>cross-entropy loss</code> function and the <code>Adam optimizer</code> over the course of 50 epochs. During this process, the U-Net learns to map RGB input images to segmentation masks, effectively identifying key game elements.
          </p>

          <p>
            The overlay shown in the segmented image demonstrates the model’s output superimposed on the original gameplay frame. This highlights the model's capability to accurately distinguish track surfaces, the car, and other visual game elements in real-time.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of the  website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
